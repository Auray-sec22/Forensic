\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{tikz}
\geometry{margin=2.5cm}

\title{ Exercices: 	\textbf{Archéologie des Régimes de Vérité }}

\date{}

\begin{document}
	\maketitle
	
	\section*{Partie 1 : Analyse Historique et Épistémologique}
	
	\subsection*{1. Analyse comparative des régimes (1990-2000 vs 2010-2020)}
	
	\textbf{Vecteurs de dominance :}
	\[
	R_{1990-2000} = (\alpha_T=0.3,\ \alpha_J=0.4,\ \alpha_S=0.0,\ \alpha_P=0.3)
	\]
	\[
	R_{2010-2020} = (\alpha_T=0.2,\ \alpha_J=0.1,\ \alpha_S=0.1,\ \alpha_P=0.6)
	\]
	
	\begin{itemize}
		\item 1990-2000 : régime dominé par le \textbf{juridique} et les \textbf{pratiques professionnelles}.
		\item 2010-2020 : régime dominé par les \textbf{pratique professionnelles et méthodologiques} et les\textbf{ transformations technologiques} .
	\end{itemize}
	
	\textbf{Discontinuités foucaldiennes :} passage de la preuve humaine/juridique vers la preuve algorithmique.  
	\textbf{Explication socio-technique :} montée d’Internet $\Rightarrow$ cadre légal ; explosion des données $\Rightarrow$ délégation aux algorithmes.  
	\textbf{Nature de la transition :} \textit{révolutionnaire}.  
	
	\subsection*{2. Étude de cas foucaldienne : Enron (2001)}
	
	\begin{itemize}
		\item \textbf{Formation discursive :} émergence de la preuve algorithmique (tri automatisé de 500.000 e-mails).
		\item \textbf{Dicible/pensable :} l’algorithme comme outil d’assistance, mais pas encore producteur de vérité.
		\item \textbf{Régime en action :} standardisation et automatisation légitimées par le droit.
	\end{itemize}
	
	\textbf{Comparaison contemporaine :} Panama Papers (2016) $\Rightarrow$ algorithmes + collaboration transnationale.  
	\textbf{Rupture :} de l’automatisation \textit{assistante} à l’automatisation \textit{décisive}.  
	
	\section*{Partie 2 : Modélisation Mathématique et Prospective}
	
	\subsection*{3. Modèle d’évolution des régimes}
	
	
	\subsection*{4. Vérification de l’accélération technologique}
	
	\textbf{Durées des régimes :}
	\[
	1970-1990 : 20 \ \text{ans}, \quad
	1990-2000 : 10 \ \text{ans}, \quad
	2000-2010 : 10 \ \text{ans}, \quad
	2010-2020 : 10 \ \text{ans}
	\]
	
	Loi : 
	\[
	\Delta t_{n+1} = k \cdot \Delta t_n \quad \text{avec } k \approx 0.5-0.7
	\]
	
	\textbf{Projection :} prochain changement probable avant 2030.
	
	\subsection*{5. Analyse du trilemme CRO}
	
	\textbf{CRO = Confidentialité, Fiabilité, Opposabilité.}
	
	\begin{itemize}
		\item 1990-2000 : priorité à l’\textbf{opposabilité}.
		\item 2010-2020 : priorité à la \textbf{fiabilité}.
		\item Futur : priorité à la \textbf{confidentialité} (preuve quantique, zero-knowledge).
	\end{itemize}
	
	\section*{Partie 3 : Investigation Historique Appliquée}
	
	\subsection*{6. Reconstruction d’une affaire : Kevin Mitnick (1995)}
	
	\begin{itemize}
		\item \textbf{Avec outils de l’époque :} logs IP, honeypots rudimentaires, analyse humaine.L’affaire Kevin Mitnick (1995) illustre les limites des enquêtes numériques des années 1990 : analyses manuelles des logs, outils rudimentaires, absence de standardisation et forte dépendance à l’expertise humaine. La vérité reposait sur l’interprétation experte et des preuves souvent circonstancielles.

		\item \textbf{Avec outils modernes :} IA, corrélation massive, attribution rapide.	Avec les outils modernes (SIEM, IA, corrélations automatiques, hashage, acquisition bit-à-bit), l’analyse est devenue plus précise, reproductible et capable de traiter de vastes volumes de données. La vérité est désormais produite par un régime hybride : algorithmes + procédures normalisées + expertise humaine.
	\end{itemize}
		Comparaison :
	1995 : régime centré sur l’expert technique, preuves fragiles et difficilement opposables.
	
	2020+ : régime algorithmique et procédural, plus fiable et reproductible, mais soulevant de nouveaux défis (transparence des algorithmes, biais, admissibilité juridique).
	
	\textbf{Impact :}Les limitations technologiques des années 1990 ont orienté la construction de la vérité vers l’autorité humaine, alors que les outils modernes renforcent l’intégrité et la précision mais déplacent la question vers la légitimité et la transparence des algorithmes. en 1995, l’expert technique incarnait l’autorité ; aujourd’hui, une partie de la légitimité est déléguée à la machine.
	
	\subsection*{7. Projet de recherche archéologique}
	
	\begin{itemize}
		\item \textbf{Trou identifié :} la période pré-numérique (1960, ARPANET).
		\item \textbf{Hypothèse :} pratiques proto-forensiques (logs papier, enregistrements système).
		\item \textbf{Méthode :} analyse des RFC originaux et archives IBM.
	\end{itemize}
	
	\subsection*{8. Analyse prospective 2030-2050}
	
	\textbf{Scénario choisi : Régime neuro-digital.}
	
	\begin{itemize}
		\item \textbf{Conditions de possibilité :} interfaces cerveau-machine, encadrement légal de la lecture/écriture neuronale.
		\item \textbf{Méthodologie :} forensique neuronale (analyse de journaux cognitifs).
		\item \textbf{Défis éthiques :} vie privée absolue, consentement cérébral.
		\item \textbf{Défis épistémologiques :} passage d’une vérité computationnelle à une vérité cognitive.
	\end{itemize}
	
\end{document}
